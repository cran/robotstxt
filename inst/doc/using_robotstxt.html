<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Peter Meissner" />

<meta name="date" content="2024-08-25" />

<title>Using Robotstxt</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">code{
white-space: pre;
}
a.sourceLine {
display: inline-block; min-height: 1.25em;
}
a.sourceLine {
pointer-events: none; color: inherit; text-decoration: inherit;
}
.sourceCode {
overflow: visible;
}
code.sourceCode {
white-space: pre;
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine {
text-indent: -1em; padding-left: 1em;
}
}
pre.numberSource a.sourceLine {
position: relative;
}
pre.numberSource a.sourceLine::before
{
content: attr(data-line-number);
position: absolute; left: -5em; text-align: right; vertical-align: baseline;
border: none; pointer-events: all;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource {
margin-left: 3em;
border-left: 1px solid #aaaaaa;
padding-left: 4px;
}
@media screen {
a.sourceLine::before {
text-decoration: underline;
color: initial;
}
}
code span.kw { color: #007020; font-weight: bold; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.bn { color: #40a070; } 
code span.fl { color: #40a070; } 
code span.ch { color: #4070a0; } 
code span.st { color: #4070a0; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.ot { color: #007020; } 
code span.al { color: #ff0000; font-weight: bold; } 
code span.fu { color: #06287e; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.cn { color: #880000; } 
code span.sc { color: #4070a0; } 
code span.vs { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.im { } 
code span.va { color: #19177c; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.bu { } 
code span.ex { } 
code span.pp { color: #bc7a00; } 
code span.at { color: #7d9029; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 90%;
line-height: 1.8;
}
#header {
text-align: center;
}
#TOC {
width: 100%;
clear: both;
margin-top: 20px;
margin-bottom: 20px;
padding: 4px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
}
#TOC .toctitle {
font-weight: bold;
font-size: 80%;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin-top: 1.5em;
margin-bottom: 1.5em;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap;
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1,h2,h3,h4,h5,h6 {
margin-top: 1.5em;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
code > span.kw { color: #555; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #d14; }
code > span.fl { color: #d14; }
code > span.ch { color: #d14; }
code > span.st { color: #d14; }
code > span.co { color: #888888; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #900; font-weight: bold; } code > span.er { color: #a61717; background-color: #e3d2d2; }
</style>




</head>

<body>




<h1 class="title toc-ignore">Using Robotstxt</h1>
<h4 class="author">Peter Meissner</h4>
<h4 class="date">2024-08-25</h4>


<div id="TOC">
<ul>
<li><a href="#description" id="toc-description">Description</a></li>
<li><a href="#robots.txt-files" id="toc-robots.txt-files">Robots.txt
files</a></li>
<li><a href="#fast-food-usage-for-the-uninterested" id="toc-fast-food-usage-for-the-uninterested">Fast food usage for the
uninterested</a></li>
<li><a href="#example-usage" id="toc-example-usage">Example Usage</a>
<ul>
<li><a href="#object-oriented-style" id="toc-object-oriented-style">object oriented style</a></li>
<li><a href="#functional-style" id="toc-functional-style">functional
style</a></li>
</ul></li>
</ul>
</div>

<div id="description" class="section level1">
<h1>Description</h1>
<p>The package provides a simple ‘robotstxt’ class and accompanying
methods to parse and check ‘robots.txt’ files. Data fields are provided
as data frames and vectors. Permissions can be checked by providing path
character vectors and optional bot names.</p>
</div>
<div id="robots.txt-files" class="section level1">
<h1>Robots.txt files</h1>
<p>Robots.txt files are a way to kindly ask webbots, spiders, crawlers,
wanderers and the like to access or not access certain parts of a
webpage. The de facto ‘standard’ never made it beyond a informal <a href="http://www.robotstxt.org/norobots-rfc.txt">“Network Working Group
INTERNET DRAFT”</a>. Nonetheless, the use of robots.txt files is
widespread (e.g. <a href="https://en.wikipedia.org/robots.txt" class="uri">https://en.wikipedia.org/robots.txt</a>, <a href="https://www.google.com/robots.txt" class="uri">https://www.google.com/robots.txt</a>) and bots from Google,
Yahoo and the like will adhere to the rules defined in robots.txt files
- although, their interpretation of those rules might differ (e.g. <a href="https://developers.google.com/search/reference/robots_txt">rules
for googlebot</a>).</p>
<p>As the name of the files already suggests robots.txt files are plain
text and always found at the root of a domain. The syntax of the files
in essence follows a <code>fieldname: value</code> scheme with optional
preceding <code>user-agent: ...</code> lines to indicate the scope of
the following rule block. Blocks are separated by blank lines and the
omission of a user-agent field (which directly corresponds to the HTTP
user-agent field) is seen as referring to all bots. <code>#</code>
serves to comment lines and parts of lines. Everything after
<code>#</code> until the end of line is regarded a comment. Possible
field names are: user-agent, disallow, allow, crawl-delay, sitemap, and
host.</p>
<p>Let us have an example file to get an idea how a robots.txt file
might look like. The file below starts with a comment line followed by a
line disallowing access to any content – everything that is contained in
root (“<code>/</code>”) – for all bots. The next block concerns GoodBot
and NiceBot. Those two get the previous permissions lifted by being
disallowed nothing. The third block is for PrettyBot. PrettyBot likes
shiny stuff and therefor gets a special permission for everything
contained in the “<code>/shinystuff/</code>” folder while all other
restrictions still hold. In the last block all bots are asked to pause
at least 5 seconds between two visits.</p>
<pre><code># this is a comment 
# a made up example of an robots.txt file

Disallow: /

User-agent: GoodBot # another comment
User-agent: NiceBot
Disallow: 

User-agent: PrettyBot
Allow: /shinystuff/

Crawl-Delay: 5</code></pre>
<p>For more information have a look at: <a href="http://www.robotstxt.org/norobots-rfc.txt" class="uri">http://www.robotstxt.org/norobots-rfc.txt</a>, where the
robots.txt file ‘standard’ is described formally. Valuable introductions
can be found at <a href="http://www.robotstxt.org/robotstxt.html" class="uri">http://www.robotstxt.org/robotstxt.html</a> as well as at <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard" class="uri">https://en.wikipedia.org/wiki/Robots_exclusion_standard</a>
- of cause.</p>
</div>
<div id="fast-food-usage-for-the-uninterested" class="section level1">
<h1>Fast food usage for the uninterested</h1>
<pre><code>library(robotstxt)
paths_allowed(&quot;http://google.com/&quot;)

## [1] TRUE

paths_allowed(&quot;http://google.com/search&quot;)

## [1] FALSE</code></pre>
</div>
<div id="example-usage" class="section level1">
<h1>Example Usage</h1>
<p>First, let us load the package. In addition we load the dplyr package
to be able to use the magrittr pipe operator <code>%&gt;%</code> and
some easy to read and remember data manipulation functions.</p>
<pre><code>library(robotstxt)
library(dplyr)</code></pre>
<div id="object-oriented-style" class="section level2">
<h2>object oriented style</h2>
<p>The first step is to create an instance of the robotstxt class
provided by the package. The instance has to be initiated via providing
either domain or the actual text of the robots.txt file. If only the
domain is provided, the robots.txt file will be downloaded
automatically. Have a look at <code>?robotstxt</code> for descriptions
of all data fields and methods as well as their parameters.</p>
<pre><code>rtxt &lt;- robotstxt(domain=&quot;wikipedia.org&quot;)</code></pre>
<p><code>rtxt</code> is of class <code>robotstxt</code>.</p>
<pre><code>class(rtxt)

## [1] &quot;robotstxt&quot;</code></pre>
<p>Printing the object lets us glance at all data fields and methods in
<code>rtxt</code> - we have access to the text as well as all common
fields. Non-standard fields are collected in <code>other</code>.</p>
<pre><code>rtxt

## $text
## [1] &quot;#\n# robots.txt for http://www.wikipedia.org/ and friends\n#\n# Please note: There are a lot of pages on this site, and there are\n# some misbehaved spiders out there that go _way_ too fast. If you&#39;re\n# irresponsible, your access to the site may be blocked.\n#\n\n# advertising-related bots:\nUser-agent: Mediapartners-Google*\n\n[... 653 lines omitted ...]&quot;
## 
## $domain
## [1] &quot;wikipedia.org&quot;
## 
## $robexclobj
## &lt;Robots Exclusion Protocol Object&gt;
## $bots
## [1] &quot;Mediapartners-Google*&quot;       &quot;IsraBot&quot;                     &quot;Orthogaffe&quot;                  &quot;UbiCrawler&quot;                 
## [5] &quot;DOC&quot;                         &quot;Zao&quot;                         &quot;&quot;                            &quot;[...  28 items omitted ...]&quot;
## 
## $comments
##   line                                                               comment
## 1    1                                                                     #
## 2    2                # robots.txt for http://www.wikipedia.org/ and friends
## 3    3                                                                     #
## 4    4   # Please note: There are a lot of pages on this site, and there are
## 5    5 # some misbehaved spiders out there that go _way_ too fast. If you&#39;re
## 6    6              # irresponsible, your access to the site may be blocked.
## 7                                                                           
## 8                                               [...  173 items omitted ...]
## 
## $permissions
##                          field             useragent value
## 1                     Disallow Mediapartners-Google*     /
## 2                     Disallow               IsraBot      
## 3                     Disallow            Orthogaffe      
## 4                     Disallow            UbiCrawler     /
## 5                     Disallow                   DOC     /
## 6                     Disallow                   Zao     /
## 7                                                         
## 8 [...  370 items omitted ...]                            
## 
## $crawl_delay
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $host
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $sitemap
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $other
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $check
## function (paths = &quot;/&quot;, bot = &quot;*&quot;) 
## {
##     spiderbar::can_fetch(obj = self$robexclobj, path = paths, 
##         user_agent = bot)
## }
## &lt;bytecode: 0x12f9629b0&gt;
## &lt;environment: 0x12f965c10&gt;
## 
## attr(,&quot;class&quot;)
## [1] &quot;robotstxt&quot;</code></pre>
<p>Checking permissions works via <code>rtxt</code>’s <code>check</code>
method by providing one or more paths. If no bot name is provided
<code>&quot;*&quot;</code> - meaning any bot - is assumed.</p>
<pre><code># checking for access permissions
rtxt$check(paths = c(&quot;/&quot;,&quot;api/&quot;), bot = &quot;*&quot;)

## [1]  TRUE FALSE

rtxt$check(paths = c(&quot;/&quot;,&quot;api/&quot;), bot = &quot;Orthogaffe&quot;)

## [1] TRUE TRUE

rtxt$check(paths = c(&quot;/&quot;,&quot;api/&quot;), bot = &quot;Mediapartners-Google*  &quot;)

## [1]  TRUE FALSE</code></pre>
</div>
<div id="functional-style" class="section level2">
<h2>functional style</h2>
<p>While working with the robotstxt class is recommended the checking
can be done with functions only as well. In the following we (1)
download the robots.txt file; (2) parse it and (3) check
permissions.</p>
<pre><code>r_text        &lt;- get_robotstxt(&quot;nytimes.com&quot;) 

r_parsed &lt;- parse_robotstxt(r_text)
r_parsed

## $useragents
## [1] &quot;*&quot;                    &quot;Mediapartners-Google&quot; &quot;AdsBot-Google&quot;        &quot;adidxbot&quot;            
## 
## $comments
## [1] line    comment
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $permissions
##       field            useragent                                 value
## 1     Allow                    *                          /ads/public/
## 2     Allow                    *             /svc/news/v3/all/pshb.rss
## 3  Disallow                    *                                 /ads/
## 4  Disallow                    *                             /adx/bin/
## 5  Disallow                    *                            /archives/
## 6  Disallow                    *                                /auth/
## 7  Disallow                    *                                /cnet/
## 8  Disallow                    *                             /college/
## 9  Disallow                    *                            /external/
## 10 Disallow                    *                      /financialtimes/
## 11 Disallow                    *                                 /idg/
## 12 Disallow                    *                             /indexes/
## 13 Disallow                    *                             /library/
## 14 Disallow                    *                    /nytimes-partners/
## 15 Disallow                    * /packages/flash/multimedia/TEMPLATES/
## 16 Disallow                    *                       /pages/college/
## 17 Disallow                    *                         /paidcontent/
## 18 Disallow                    *                            /partners/
## 19 Disallow                    *                  /restaurants/search*
## 20 Disallow                    *                             /reuters/
## 21 Disallow                    *                             /register
## 22 Disallow                    *                           /thestreet/
## 23 Disallow                    *                                  /svc
## 24 Disallow                    *                     /video/embedded/*
## 25 Disallow                    *                        /web-services/
## 26 Disallow                    *               /gst/travel/travsearch*
## 27 Disallow Mediapartners-Google                  /restaurants/search*
## 28 Disallow        AdsBot-Google                  /restaurants/search*
## 29 Disallow             adidxbot                  /restaurants/search*
## 
## $crawl_delay
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $sitemap
##     field useragent                                                                  value
## 1 Sitemap         * http://spiderbites.nytimes.com/sitemaps/www.nytimes.com/sitemap.xml.gz
## 2 Sitemap         *            http://www.nytimes.com/sitemaps/sitemap_news/sitemap.xml.gz
## 3 Sitemap         *   http://spiderbites.nytimes.com/sitemaps/sitemap_video/sitemap.xml.gz
## 
## $host
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)
## 
## $other
## [1] field     useragent value    
## &lt;0 rows&gt; (or 0-length row.names)

paths_allowed(
  paths  = c(&quot;images/&quot;,&quot;/search&quot;), 
  domain = c(&quot;wikipedia.org&quot;, &quot;google.com&quot;),
  bot    = &quot;Orthogaffe&quot;
)

##  wikipedia.org                       google.com

## [1]  TRUE FALSE</code></pre>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
